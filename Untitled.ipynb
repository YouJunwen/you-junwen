{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: /user/st084081/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/user/st084081/proxy/4040/jobs/\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcb53fbe4f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import sklearn\n",
    "import socket\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return \"{}proxy/{}/jobs/\".format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "# small fix to enable UI views\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "# spark configurtion in local regime \n",
    "conf = SparkConf().set('spark.master', 'local[*]').set('spark.driver.memory', '8g')\n",
    "\n",
    "#some needed objects\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                text| id|\n",
      "+--------------------+---+\n",
      "|The Project Guten...|  0|\n",
      "|                    |  1|\n",
      "|This eBook is for...|  2|\n",
      "|most other parts ...|  3|\n",
      "|whatsoever. You m...|  4|\n",
      "|of the Project Gu...|  5|\n",
      "|www.gutenberg.org...|  6|\n",
      "|will have to chec...|  7|\n",
      "|   using this eBook.|  8|\n",
      "|                    |  9|\n",
      "| Title: Frankenstein| 10|\n",
      "|       or, The Mo...| 11|\n",
      "|                    | 12|\n",
      "|Author: Mary Woll...| 13|\n",
      "|                    | 14|\n",
      "|Release Date: 31,...| 15|\n",
      "|[Most recently up...| 16|\n",
      "|                    | 17|\n",
      "|   Language: English| 18|\n",
      "|                    | 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_prefix = \"malyutin_demo_hw1\"\n",
    "\n",
    "filepath = \"file:///home/jovyan/shared/lectures_folder/84-0.txt\"\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "dataframe = sc.textFile(f\"{filepath}\")\\\n",
    "    .map(lambda x: (x,))\\\n",
    "    .toDF()\\\n",
    "    .select(F.col(\"_1\").alias(\"text\"))\\\n",
    "    .withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import string\n",
    "import re\n",
    "\n",
    "def process_string(data):\n",
    "\n",
    "    punct_removed = re.sub(r'[^\\w\\s]','',data)\n",
    "    words = punct_removed.lower().split(\" \")\n",
    "    \n",
    "    \n",
    "    return list(filter(lambda x: len(x) > 0, words))\n",
    "\n",
    "\n",
    "process_string_udf = udf(lambda z: process_string(z), ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            by_words|\n",
      "+--------------------+\n",
      "|[the, project, gu...|\n",
      "|[this, ebook, is,...|\n",
      "|[most, other, par...|\n",
      "|[whatsoever, you,...|\n",
      "|[of, the, project...|\n",
      "|[wwwgutenbergorg,...|\n",
      "|[will, have, to, ...|\n",
      "|[using, this, ebook]|\n",
      "|[title, frankenst...|\n",
      "|[or, the, modern,...|\n",
      "|[author, mary, wo...|\n",
      "|[release, date, 3...|\n",
      "|[most, recently, ...|\n",
      "| [language, english]|\n",
      "|[character, set, ...|\n",
      "|[produced, by, ju...|\n",
      "|[further, correct...|\n",
      "|[start, of, the, ...|\n",
      "|[or, the, modern,...|\n",
      "|[by, mary, wollst...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "by_words = dataframe\\\n",
    "    .select(process_string_udf(F.col(\"text\")).alias(\"by_words\"))\\\n",
    "    .where(F.size(F.col(\"by_words\")) > 1)\n",
    "\n",
    "\n",
    "by_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the| 4377|\n",
      "|  and| 3038|\n",
      "|    i| 2840|\n",
      "|   of| 2762|\n",
      "|   to| 2169|\n",
      "|   my| 1773|\n",
      "|    a| 1441|\n",
      "|   in| 1187|\n",
      "| that| 1029|\n",
      "|  was| 1022|\n",
      "|   me|  861|\n",
      "| with|  713|\n",
      "|  but|  689|\n",
      "|  had|  686|\n",
      "|  you|  644|\n",
      "|   he|  604|\n",
      "|which|  565|\n",
      "|   it|  561|\n",
      "|  not|  536|\n",
      "|   as|  536|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_words_count = by_words.select(F.explode(F.col(\"by_words\")).alias(\"word\"))\\\n",
    "    .groupBy(F.col(\"word\")).count()\\\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    "\n",
    "by_words_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_words_count.repartition(1)\\\n",
    "    .write.mode(\"overwrite\").csv(f\"df_by_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' Letter 2', 40),\n",
       " (' Letter 3', 41),\n",
       " (' Letter 4', 42),\n",
       " (' Chapter 1', 43),\n",
       " (' Chapter 2', 44),\n",
       " (' Chapter 3', 45),\n",
       " (' Chapter 4', 46),\n",
       " (' Chapter 5', 47),\n",
       " (' Chapter 6', 48),\n",
       " (' Chapter 7', 49),\n",
       " ('Inspirited by this wind of promise, my daydreams become more fervent', 90),\n",
       " ('and vivid. I try in vain to be persuaded that the pole is the seat of', 91),\n",
       " ('frost and desolation; it ever presents itself to my imagination as the',\n",
       "  92),\n",
       " ('region of beauty and delight. There, Margaret, the sun is for ever', 93),\n",
       " ('visible, its broad disk just skirting the horizon and diffusing a', 94),\n",
       " ('perpetual splendour. There—for with your leave, my sister, I will put', 95),\n",
       " ('some trust in preceding navigators—there snow and frost are banished;', 96),\n",
       " ('and, sailing over a calm sea, we may be wafted to a land surpassing in',\n",
       "  97),\n",
       " ('wonders and in beauty every region hitherto discovered on the habitable',\n",
       "  98),\n",
       " ('globe. Its productions and features may be without example, as the', 99)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddText = sc.textFile(f\"{filepath}\").repartition(1).zipWithIndex().repartition(5)\n",
    "\n",
    "\n",
    "\n",
    "rddText.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
